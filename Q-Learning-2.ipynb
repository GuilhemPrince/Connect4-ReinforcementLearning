{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e540a7-ac80-4cc4-a0c2-43679682283e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-iqk44qm9 because the default path (/home/epic_euclid/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from pettingzoo.classic import connect_four_v3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "from IPython.display import clear_output\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd701f65-d576-4373-834a-e8db76e88be0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Dimensions du tableau\n",
    "height=6\n",
    "width=7\n",
    "\n",
    "#Fonction qui initialise la matrice Q\n",
    "def initialize_Q(): #Initialise la matrice Q (que des zéros) ####################################\n",
    "    return np.zeros((height, width, height, width, width)) \n",
    "def find_best_legal(arr, mask):\n",
    "    legal_gains=arr*mask\n",
    "    best_action=np.argmax(legal_gains)\n",
    "    if legal_gains[best_action] != 0:\n",
    "        return best_action\n",
    "    else :\n",
    "        action=random.randint(0,6)\n",
    "        while mask[action]==0:\n",
    "            action=random.randint(0,6)\n",
    "        return action\n",
    "def left_legal(actions,mask):\n",
    "    for i in range(len(actions)):\n",
    "        if not (mask[i]==0) :\n",
    "            return i\n",
    "        \n",
    "class Player:\n",
    "    def __init__(self, _id, Q, epsilon=0.7, rng=None):  ############################\n",
    "        self.name = 'Player '+str(_id)\n",
    "        self.Q=initialize_Q()\n",
    "        self.st=[None,None,None,None] #Etat\n",
    "        self.last_st=[None,None,None,None] \n",
    "        self.last_action=None\n",
    "        self._id=_id #Identifiant\n",
    "        self.epsilon=epsilon\n",
    "        self.left=False\n",
    "        \n",
    "    def get_action(self, obs, epsilon=None):\n",
    "        mask=obs[\"action_mask\"]\n",
    "        if self.left:\n",
    "            actions=[0,1,2,3,4,5,6]\n",
    "            return left_legal(actions,mask)\n",
    "        (a,b,c,d)=self.st\n",
    "        if (random.uniform(0,1)<epsilon) or ((a,b) == (None,None)): #Exploration ou premier coupde la partie\n",
    "            action=random.randint(0,6)\n",
    "            while mask[action]==0:\n",
    "                action=random.randint(0,6)\n",
    "        else:\n",
    "            (x1, y1, x2, y2)=self.st\n",
    "            Q=self.Q\n",
    "            #print(x,y)\n",
    "            action=find_best_legal(Q[x1, y1, x2, y2], mask) #On choisit la meilleure action parmi celles qu'il peut jouer (meilleure dans le sens maximise Q)\n",
    "        return action\n",
    "    \n",
    "    def highest_not_null(self, action, obs, terminated): #Fonction auxiliaire à la fonction pour déterminer l'état\n",
    "        if not terminated:\n",
    "            idx=1\n",
    "        else:\n",
    "            idx=0\n",
    "        for i in range(height):\n",
    "            if obs[\"observation\"][:, action, idx][i] != 0: \n",
    "                return i\n",
    "\n",
    "    def update_state(self,position):\n",
    "        self.st[0]=self.st[2]\n",
    "        self.st[1]=self.st[3]\n",
    "        self.st[2]=position[0]\n",
    "        self.st[3]=position[1]\n",
    "        \n",
    "    def update_Q(self, obs, action, alpha, gamma,show=False): #fonction qui update Q selon \n",
    "        next_action=self.get_action(obs, epsilon=0.0)\n",
    "        [a,b,c,d]=self.st.copy()\n",
    "        #print(\"state : \",a,b,c,d)\n",
    "        [x,y,z,t]=self.last_st.copy()\n",
    "        #print(\"last state : \",x,y,z,t)\n",
    "        Q=self.Q\n",
    "        pic=Q[x,y,z,t].copy() #Garder en mémoire les anciennes récompenses pour chaque action de l'état (x,y) (pour la visualisation)\n",
    "        self.Q[x,y,z,t][action]=(1-alpha)*Q[x,y,z,t][action] + alpha*gamma*Q[a,b,c,d][next_action]#-Q[x,y][action]) #modif update_Q\n",
    "        if show:\n",
    "            print(\"Pour l'état ( \"+str(x)+\" , \"+str(y)+\" , \"+str(z)+\" , \"+str(t)+\" ), voici les anciennes récompenses : \"+str(pic)+\" et voici les nouvelles : \"+str(self.Q[x,y,z,t]))\n",
    "    def update_Q_reward(self, obs,r, action, alpha, gamma,show=False): #fonction qui update Q selon \n",
    "        [x,y,z,t]=self.st.copy()\n",
    "        Q=self.Q\n",
    "        pic=Q[x,y,z,t].copy() #Garder en mémoire les anciennes récompenses pour chaque action de l'état (x,y) (pour la visualisation)\n",
    "        self.Q[x,y,z,t][action]=(1-alpha)*Q[x,y,z,t][action] + alpha*r \n",
    "        if show:\n",
    "            print(\"Pour l'état ( \"+str(x)+\" , \"+str(y)+\" , \"+str(z)+\" , \"+str(t)+\" ), voici les anciennes récompenses : \"+str(pic)+\" et voici les nouvelles : \"+str(self.Q[x,y,z,t]))\n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.st=[None,None,None,None]\n",
    "        self.last_st=[None,None,None,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "126b819d-f9c2-4ecd-b3bc-779ab018b749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def play_game(env, agent0, agent1, alpha, gamma, display=False): #e epsilon coeff d'exploration\n",
    "    done = False\n",
    "    env.reset()\n",
    "    obs, _, _, _, _ = env.last()\n",
    "    l=[agent0, agent1]\n",
    "    random.shuffle(l)\n",
    "    new_state=(None,None,None,None)\n",
    "    while not done:\n",
    "        for i, agent in enumerate(l):\n",
    "            position=(new_state[2],new_state[3]) #Position du dernier coup de l'adversaire\n",
    "            agent.update_state(position) #Etat de l'agent mis à jour avec ce qu'a joué l'adversaire\n",
    "            last_state= agent.st.copy() #Récupération de l'état de l'agent\n",
    "            [a,b,c,d]=agent.last_st.copy()\n",
    "            if (a!=None) and (b!=None) :#and (c!=None) and (d !=None): #Mise à jour des récompenses dans Q\n",
    "                agent.update_Q(obs, agent.last_action, alpha, gamma, show=False) ############ A Regarder\n",
    "            \n",
    "            #Trouver l'action\n",
    "            e=agent.epsilon\n",
    "            action = agent.get_action(obs, epsilon=e) \n",
    "            agent.last_action=action\n",
    "            env.step(action) #Préciser à l'environnement pettingZoo que l'action prise\n",
    "            obs, reward, terminated, _, _ = env.last()\n",
    "            \n",
    "            if reward==1:\n",
    "                agent.update_Q_reward(obs,reward, action, alpha, gamma,show=False)\n",
    "            position=(agent.highest_not_null(action, obs, terminated),action) #Position du coup de l'agent\n",
    "\n",
    "            if display:\n",
    "                clear_output(wait=True)\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "            done = terminated #Déroulement fini\n",
    "            \n",
    "            if np.sum(obs[\"action_mask\"]) == 0: #Vérification d'une éventuelle égalité\n",
    "                if display: \n",
    "                    print('Draw')\n",
    "                done=True\n",
    "                break\n",
    "                \n",
    "            agent.last_st=last_state\n",
    "            agent.update_state(position) #Mise à jour de l'état de l'agent (l'état est intermédiaire avant l'udating en début de boucle)\n",
    "            new_state=agent.st.copy()\n",
    "            if done: #Victoire d'un des joueurs\n",
    "                if display:\n",
    "                    print(f\"{agent.name} : won\")\n",
    "                    print(obs['observation'][:, :, 0]- obs['observation'][:, :, 1])\n",
    "                    print('last_state : ', last_state)\n",
    "                    print('action : ', action)\n",
    "                break       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a4084c0-81c6-496e-962e-0267db1af406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def play_game_count(env, agent0, agent1, alpha, gamma, display=False, dic={}): #e epsilon coeff d'exploration\n",
    "    done = False\n",
    "    env.reset()\n",
    "    obs, _, _, _, _ = env.last()\n",
    "    l=[agent0, agent1]\n",
    "    random.shuffle(l)\n",
    "    new_state=(None,None,None,None)\n",
    "    while not done:\n",
    "        #print(\"\")\n",
    "        #print(\"\")\n",
    "        for i, agent in enumerate(l):\n",
    "            position=(new_state[2],new_state[3]) #Position du dernier coup de l'adversaire\n",
    "            agent.update_state(position) #Etat de l'agent mis à jour avec ce qu'a joué l'adversaire\n",
    "            last_state= agent.st.copy() #Récupération de l'état de l'agent\n",
    "            [a,b,c,d]=agent.last_st.copy()\n",
    "            if (a!=None) and (b!=None) :#and (c!=None) and (d !=None): #Mise à jour des récompenses dans Q\n",
    "                agent.update_Q(obs, agent.last_action, alpha, gamma, show=False) ############ A Regarder\n",
    "            \n",
    "            #Trouver l'action\n",
    "            e=agent.epsilon\n",
    "            action = agent.get_action(obs, epsilon=e) \n",
    "            agent.last_action=action\n",
    "            env.step(action) #Préciser à l'environnement pettingZoo que l'action prise\n",
    "            obs, reward, terminated, _, _ = env.last()\n",
    "            if reward==1:\n",
    "                agent.update_Q_reward(obs,reward, action, alpha, gamma,show=False)\n",
    "            position=(agent.highest_not_null(action, obs, terminated),action) #Position du coup de l'agent\n",
    "\n",
    "            if display:\n",
    "                clear_output(wait=True)\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "            done = terminated #Déroulement fini\n",
    "            if np.sum(obs[\"action_mask\"]) == 0: #Vérification d'une éventuelle égalité\n",
    "                if display: \n",
    "                    print('Draw')\n",
    "                done=True\n",
    "                result='Draw'\n",
    "                break\n",
    "                \n",
    "            agent.last_st=last_state\n",
    "            agent.update_state(position) #Mise à jour de l'état de l'agent (l'état est intermédiaire avant l'udating en début de boucle)\n",
    "            new_state=agent.st.copy()\n",
    "            if done: #Victoire d'un des joueurs\n",
    "                if display:\n",
    "                    print(f\"{agent.name} : won\")\n",
    "                    print(obs['observation'][:, :, 0]- obs['observation'][:, :, 1])\n",
    "                    print('last_state : ', last_state)\n",
    "                    print('action : ', action)\n",
    "                done=True\n",
    "                result=agent.name\n",
    "                break       \n",
    "    dic[result]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "912628ff-cd33-4d15-8304-385403897126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multiple_games_count(env, agent0, agent1, alpha, gamma , count):\n",
    "    dic={}\n",
    "    dic['Draw']=0\n",
    "    dic[agent0.name]=0\n",
    "    dic[agent1.name]=0\n",
    "    for i in tqdm(range(count)):\n",
    "        agent0.reset_states()\n",
    "        agent1.reset_states()\n",
    "        play_game_count(env, agent0, agent1, alpha, gamma, display=False, dic=dic)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815925cc-c03f-46fa-8981-9b5d0da49c51",
   "metadata": {},
   "source": [
    "Entraînement et évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9272baa4-3e4d-4a0c-8f8d-a23a999bfd84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = connect_four_v3.env(render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "agent0=Player(_id=1, Q=initialize_Q(), epsilon=0.3) #Il explore 30% du tps\n",
    "random_player=Player(_id=2, Q=initialize_Q(), epsilon =1) #Random explore 100% du tps\n",
    "epsilon_training_mate=1\n",
    "training_mate=Player(_id=2, Q=initialize_Q(), epsilon =epsilon_training_mate)\n",
    "alpha=0.1\n",
    "gamma=0.9\n",
    "loop=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "498ae18e-0374-4859-b5ae-4657f324a34a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [04:29<00:00, 334.09it/s]\n"
     ]
    }
   ],
   "source": [
    "#training_mate.left=True\n",
    "for i in tqdm(range(9*loop)): #Entraînement\n",
    "    agent0.reset_states()\n",
    "    training_mate.reset_states()\n",
    "    play_game(env, agent0, training_mate, alpha, gamma , display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e47196b-335d-438d-a8eb-5663d282dfdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:49<00:00, 800.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Draw': 0, 'Player 1': 14204, 'Player 2': 25796}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent0.epsilon=0 #On le passe en mode exploitation\n",
    "random_player.left=True\n",
    "multiple_games_count(env, agent0, random_player, alpha, gamma , 4*loop)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f7d818-f2b3-4a78-841a-e83bb4f75607",
   "metadata": {
    "tags": []
   },
   "source": [
    "Parameters Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9152a81d-1e74-477a-ae05-c1ca2e75eaf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing :  0.001 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:24<00:00, 277.09it/s]\n",
      "100%|██████████| 40000/40000 [02:23<00:00, 278.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing :  0.001 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:36<00:00, 254.86it/s]\n",
      "100%|██████████| 40000/40000 [02:28<00:00, 269.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing :  0.01 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:16<00:00, 293.64it/s]\n",
      "100%|██████████| 40000/40000 [02:15<00:00, 294.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing :  0.01 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:22<00:00, 280.89it/s]\n",
      "100%|██████████| 40000/40000 [02:18<00:00, 289.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing :  0.1 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:05<00:00, 319.55it/s]\n",
      "100%|██████████| 40000/40000 [01:54<00:00, 349.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing :  0.1 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:14<00:00, 297.81it/s]\n",
      "100%|██████████| 40000/40000 [01:57<00:00, 341.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(0.001, 0.5): {'Draw': 83, 'Player 1': 25188, 'Player Random': 14729},\n",
       " (0.001, 0.9): {'Draw': 82, 'Player 1': 23988, 'Player Random': 15930},\n",
       " (0.01, 0.5): {'Draw': 57, 'Player 1': 25239, 'Player Random': 14704},\n",
       " (0.01, 0.9): {'Draw': 76, 'Player 1': 24632, 'Player Random': 15292},\n",
       " (0.1, 0.5): {'Draw': 26, 'Player 1': 28675, 'Player Random': 11299},\n",
       " (0.1, 0.9): {'Draw': 40, 'Player 1': 28845, 'Player Random': 11115}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_={}\n",
    "for alpha in [0.001, 0.01, 0.1]:\n",
    "    for gamma in [0.5,0.9]:\n",
    "        print(\"Processing : \",alpha,gamma)\n",
    "        env = connect_four_v3.env(render_mode=\"rgb_array\")\n",
    "        env.reset()\n",
    "        agent0=Player(_id=1, Q=initialize_Q(), epsilon=0.3)\n",
    "        training_mate=Player(_id=2, Q=initialize_Q(), epsilon=1)\n",
    "        random_player=Player(_id=\"Random\", Q=initialize_Q(), epsilon=1)\n",
    "        e=0.0\n",
    "        loop=40000\n",
    "        for i in tqdm(range(loop)): #training\n",
    "            agent0.reset_states()\n",
    "            training_mate.reset_states()\n",
    "            play_game(env, agent0, training_mate, alpha, gamma , display=False)\n",
    "        dic=multiple_games_count(env, agent0, random_player, alpha, gamma , loop) \n",
    "        dic_[(alpha,gamma)]=dic\n",
    "dic_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
