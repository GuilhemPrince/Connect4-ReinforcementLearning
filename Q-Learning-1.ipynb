{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e540a7-ac80-4cc4-a0c2-43679682283e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-wfqpdj_k because the default path (/home/epic_euclid/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from pettingzoo.classic import connect_four_v3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "from IPython.display import clear_output\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd701f65-d576-4373-834a-e8db76e88be0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Dimensions du tableau\n",
    "height=6\n",
    "width=7\n",
    "\n",
    "#Fonction qui initialise la matrice Q\n",
    "def initialize_Q(): #Initialise la matrice Q (que des zéros)\n",
    "    return np.zeros((height, width, width)) \n",
    "def find_best_legal(arr, mask):\n",
    "    legal_gains=arr*mask\n",
    "    best_action=np.argmax(legal_gains)\n",
    "    if legal_gains[best_action] != 0:\n",
    "        return best_action\n",
    "    else :\n",
    "        action=random.randint(0,6)\n",
    "        while mask[action]==0:\n",
    "            action=random.randint(0,6)\n",
    "        return action\n",
    "def left_legal(actions,mask):\n",
    "    for i in range(len(actions)):\n",
    "        if not (mask[i]==0) :\n",
    "            return i\n",
    "\n",
    "class Player:\n",
    "    def __init__(self, _id, Q, epsilon=0.7, rng=None): \n",
    "        self.name = 'Player '+str(_id)\n",
    "        self.Q=initialize_Q()\n",
    "        self.st=(None,None) #Etat\n",
    "        self._id=_id #Identifiant\n",
    "        self.epsilon=epsilon\n",
    "        self.left=False\n",
    "        \n",
    "    def get_action(self, obs, epsilon=None):\n",
    "        mask=obs[\"action_mask\"]\n",
    "        if self.left:\n",
    "            actions=[0,1,2,3,4,5,6]\n",
    "            return left_legal(actions,mask)\n",
    "        if (random.uniform(0,1)<epsilon) or (self.st == (None,None)): #Exploration ou premier coupde la partie\n",
    "            action=random.randint(0,6)\n",
    "            while mask[action]==0:\n",
    "                action=random.randint(0,6)\n",
    "        else:\n",
    "            (x,y)=self.st\n",
    "            Q=self.Q\n",
    "            #print(x,y)\n",
    "            action=find_best_legal(Q[x,y], mask) #On choisit la meilleure action parmi celles qu'il peut jouer (meilleure dans le sens maximise Q)\n",
    "        return action\n",
    "    \n",
    "    def highest_not_null(self, action, obs, terminated): #Fonction auxiliaire à la fonction pour déterminer l'état\n",
    "        if not terminated:\n",
    "            idx=1\n",
    "        else:\n",
    "            idx=0\n",
    "        for i in range(height):\n",
    "            if obs[\"observation\"][:, action, idx][i] != 0: \n",
    "                return i\n",
    "\n",
    "    def update_state(self,action, obs, terminated): #Fonction qui update l'état du joueur (trouve en fait le dernier coup joué)\n",
    "        y=action\n",
    "        x=self.highest_not_null(action, obs, terminated)\n",
    "        self.st=(x,y)\n",
    "    def update_Q(self, last_state, obs, action, r, alpha, gamma,show=False): #fonction qui update Q selon \n",
    "        next_action=self.get_action(obs, epsilon=0.0)\n",
    "        (a,b)=self.st\n",
    "        (x,y)=last_state\n",
    "        Q=self.Q\n",
    "        pic=Q[x,y] #Garder en mémoire les anciennes récompenses pour chaque action de l'état (x,y) (pour la visualisation)\n",
    "        self.Q[x,y][action]=(1-alpha)*Q[x,y][action] + alpha*(r + gamma*Q[a,b][next_action])#-Q[x,y][action]) #modif update_Q\n",
    "        if show:\n",
    "            print(\"Pour l'état ( \"+str(x)+\" , \"+str(y)+\" ), voici les anciennes récompenses : \"+str(pic)+\" et voici les nouvelles : \"+str(self.Q[x,y]))\n",
    "\n",
    "def play_game(env, agent0, agent1, alpha, gamma, display=False): #e epsilon coeff d'exploration\n",
    "    done = False\n",
    "    env.reset()\n",
    "    obs, _, _, _, _ = env.last()\n",
    "    l=[agent0, agent1]\n",
    "    random.shuffle(l)\n",
    "    while not done:\n",
    "        #print(\"\")\n",
    "        #print(\"\")\n",
    "        for i, agent in enumerate(l):\n",
    "            last_state=agent.st\n",
    "            e=agent.epsilon\n",
    "            action = agent.get_action(obs, epsilon=e) #Epsilon = taux d'exploration #Il s'agit de obs du point de vue de l'autre joueur mais on veut que le mask\n",
    "            env.step(action) #Préciser à l'environnement pettingZoo que l'action prise\n",
    "            obs, reward, terminated, _, _ = env.last()\n",
    "            agent.update_state(action, obs, terminated) #Mise à jour de l'état de l'agent\n",
    "\n",
    "            if display:\n",
    "                clear_output(wait=True)\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "            done = terminated #Déroulement fini\n",
    "            if np.sum(obs[\"action_mask\"]) == 0: #Vérification d'une éventuelle égalité\n",
    "                if display: \n",
    "                    print('Draw')\n",
    "                done=True\n",
    "                break\n",
    "                \n",
    "            if last_state != (None, None) : #Mise à jour des récompenses dans Q\n",
    "                agent.update_Q(last_state, obs, action, reward, alpha, gamma, show=False)\n",
    "            \n",
    "            if done: #Victoire d'un des joueurs\n",
    "                if display:\n",
    "                    print(f\"{agent.name} : won\")\n",
    "                    print(obs['observation'][:, :, 0]- obs['observation'][:, :, 1])\n",
    "                    print('last_state : ', last_state)\n",
    "                    print('action : ', action)\n",
    "                done=True\n",
    "                break       \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f907a38-18d6-4d62-89d0-85bb48739741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def play_game_count(env, agent0, agent1, alpha, gamma, display=False, dic={}): #e epsilon coeff d'exploration\n",
    "    done = False\n",
    "    env.reset()\n",
    "    obs, _, _, _, _ = env.last()\n",
    "    l=[agent0, agent1]\n",
    "    random.shuffle(l)\n",
    "    while not done:\n",
    "        #print(\"\")\n",
    "        #print(\"\")\n",
    "        for i, agent in enumerate(l):\n",
    "            #print(\"\")\n",
    "            #print(\"Player : \"+str(agent._id))\n",
    "                            #print(type(agent))\n",
    "            e=agent.epsilon\n",
    "            last_state=agent.st\n",
    "            #print('last_state : ',last_state)\n",
    "            action = agent.get_action(obs, epsilon=e) #Epsilon = taux d'exploration\n",
    "            #print('action : ',action)\n",
    "            #print(action)\n",
    "            env.step(action) #Préciser à l'environnement pettingZoo que l'action prise\n",
    "            obs, reward, terminated, _, _ = env.last()\n",
    "            #print(\"obs : \",obs['observation'][:, :, 1])\n",
    "            agent.update_state(action, obs, terminated) #Mise à jour de l'état de l'agent\n",
    "\n",
    "            if display:\n",
    "                clear_output(wait=True)\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "            done = terminated #Déroulement fini\n",
    "            if np.sum(obs[\"action_mask\"]) == 0: #Vérification d'une éventuelle égalité\n",
    "                if display: \n",
    "                    print('Draw')\n",
    "                done=True\n",
    "                result='Draw'\n",
    "                break\n",
    "\n",
    "            #if last_state != (None, None) : #Mise à jour des récompenses dans Q #Pas de mise à jour des récompenses pour l'évaluation\n",
    "                #agent.update_Q(last_state, obs, action, reward, alpha, gamma, show=False)\n",
    "\n",
    "            if done: #Victoire d'un des joueurs\n",
    "                if display:\n",
    "                    print(f\"{agent.name} : won\")\n",
    "                    print(obs['observation'][:, :, 0]- obs['observation'][:, :, 1])\n",
    "                    print('last_state : ', last_state)\n",
    "                    print('action : ', action)\n",
    "                result=agent.name\n",
    "                break\n",
    "    dic[result]+=1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9869af-9fa1-435a-bd73-d00035400c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multiple_games_count(env, agent0, agent1, alpha, gamma , count):\n",
    "    dic={}\n",
    "    dic['Draw']=0\n",
    "    dic[agent0.name]=0\n",
    "    dic[agent1.name]=0\n",
    "    for i in tqdm(range(count)):\n",
    "        play_game_count(env, agent0, agent1, alpha, gamma, display=False, dic=dic)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815925cc-c03f-46fa-8981-9b5d0da49c51",
   "metadata": {},
   "source": [
    "Entrainement et Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9272baa4-3e4d-4a0c-8f8d-a23a999bfd84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = connect_four_v3.env(render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "agent0=Player(_id=1, Q=initialize_Q(), epsilon=0.3) #Il explore 30% du tps\n",
    "random_player=Player(_id=2, Q=initialize_Q(), epsilon =1) #Random explore 100% du tps\n",
    "epsilon_training_mate=1\n",
    "training_mate=Player(_id=2, Q=initialize_Q(), epsilon =epsilon_training_mate)\n",
    "alpha=0.01\n",
    "gamma=0.5\n",
    "loop=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "498ae18e-0374-4859-b5ae-4657f324a34a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:55<00:00, 357.95it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(2*loop)): #Entraînement\n",
    "    play_game(env, agent0, training_mate, alpha, gamma , display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e47196b-335d-438d-a8eb-5663d282dfdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:18<00:00, 545.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Draw': 0, 'Player 1': 8595, 'Player 2': 1405}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent0.epsilon=0 #On le passe en mode exploitation\n",
    "random_player.left=False\n",
    "multiple_games_count(env, agent0, random_player, alpha, gamma , loop)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f7d818-f2b3-4a78-841a-e83bb4f75607",
   "metadata": {
    "tags": []
   },
   "source": [
    "Parameters Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9152a81d-1e74-477a-ae05-c1ca2e75eaf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing :  0.001 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:24<00:00, 412.94it/s]\n",
      "100%|██████████| 10000/10000 [00:22<00:00, 450.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing :  0.001 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:25<00:00, 388.05it/s]\n",
      "100%|██████████| 10000/10000 [00:22<00:00, 442.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing :  0.1 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:21<00:00, 457.57it/s]\n",
      "100%|██████████| 10000/10000 [00:17<00:00, 580.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing :  0.1 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:20<00:00, 484.27it/s]\n",
      "100%|██████████| 10000/10000 [00:17<00:00, 574.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing :  0.1 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:20<00:00, 476.42it/s]\n",
      "100%|██████████| 10000/10000 [00:18<00:00, 547.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing :  0.1 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:22<00:00, 444.41it/s]\n",
      "100%|██████████| 10000/10000 [00:18<00:00, 533.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0.001, 0.5): {'Draw': 7, 'Player 1': 6148, 'Player 2': 3845}, (0.001, 0.9): {'Draw': 18, 'Player 1': 5926, 'Player 2': 4056}, (0.1, 0.5): {'Draw': 11, 'Player 1': 7654, 'Player 2': 2335}, (0.1, 0.9): {'Draw': 8, 'Player 1': 7418, 'Player 2': 2574}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dic_={}\n",
    "for alpha in [0.001, 0.01, 0.1]:\n",
    "    for gamma in [0.5,0.9]:\n",
    "        print(\"Processing : \",alpha,gamma)\n",
    "        env = connect_four_v3.env(render_mode=\"rgb_array\")\n",
    "        env.reset()\n",
    "        agent0=Player(_id=1, Q=initialize_Q(), epsilon=0.3)\n",
    "        training_mate=Player(_id=2, Q=initialize_Q(), epsilon=1)\n",
    "        agent_random=Player(_id=\"Random\", Q=initialize_Q(), epsilon=1)\n",
    "        e=0.0\n",
    "        loop=10000\n",
    "        for i in tqdm(range(loop)): #training\n",
    "            play_game(env, agent0, training_mate, alpha, gamma , display=False)\n",
    "        dic=multiple_games_count(env, agent0, random_player, alpha, gamma , loop) \n",
    "        dic_[(alpha,gamma)]=dic\n",
    "dic_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
